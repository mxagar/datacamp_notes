{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f38007ed-0d58-4422-a729-accba315cbca",
   "metadata": {},
   "source": [
    "# 3. Fine-Tuning XGBoost Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295fcf3a-58e8-4df8-a2c6-a3cc2002edcd",
   "metadata": {},
   "source": [
    "These are my personal notes of the Datacamp course [Extreme Gradient Boosting with XGBoost](https://app.datacamp.com/learn/courses/extreme-gradient-boosting-with-xgboost).\n",
    "\n",
    "The course has 4 main sections:\n",
    "\n",
    "1. Classification\n",
    "2. Regression\n",
    "3. **Fine-tuning XGBoost**: the current notebook.\n",
    "4. Using XGBoost in Pipelines\n",
    "\n",
    "XGBoost is an implementation of the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) algorithm in C++ which has bindings to other languages, such as Python. It has the following properties:\n",
    "\n",
    "- Fast.\n",
    "- Best performance.\n",
    "- Parallelizable, on a computer and across the network. So it can work with huge datasets distributed on several nodes/GPUs.\n",
    "- We can use it for classification and regression.\n",
    "- The [Python API](https://xgboost.readthedocs.io/en/stable/python/python_api.html) is easy to use and has two major flavors or sub-APIs:\n",
    "  - The **Scikit-Learn API**: We instantiate `XGBRegressor()` or `XGBClassifier` and then we can `fit()` and `predict()`, using the typical Scikit-Learn parameters; we can even use those objects with other Scikit-Learn modules, such as `GridSearchCV`.\n",
    "  - The **Learning API**: The native XGBoost Python API requires to convert the dataframes into `DMatrix` objects first; then, we have powerful methods which allow for tuning many parameters: `xgb.cv()`, `xgb.train()`. The native/learning API is very easy to use. **Note: the parameter names are different compared to the Scikit-Learn API!**\n",
    "\n",
    "Classification is the original supervised learning problem addressed by XGBoost, although it can also handle regression problems.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```python\n",
    "# PIP\n",
    "pip install xgboost\n",
    "\n",
    "# Conda: General\n",
    "conda install -c conda-forge py-xgboost\n",
    "\n",
    "# Conda: CPU only\n",
    "conda install -c conda-forge py-xgboost-cpu\n",
    "\n",
    "# Conda: Use NVIDIA GPU: Linux x86_64\n",
    "conda install -c conda-forge py-xgboost-gpu\n",
    "\n",
    "# For tree visualization\n",
    "pip install graphviz\n",
    "```\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [3.1 Manual Hyperparameter Selection](#3.1-Manual-Hyperparameter-Selection)\n",
    "    - Example of Manual Parameter Selection\n",
    "    - Effect of Varying a Hyperparameter: Number of Boosting Rounds\n",
    "    - Automated Selection with Early Stopping\n",
    "- [3.2 Most Common Tunable Hyperparameters](#3.2-Most-Common-Tunable-Hyperparameters)\n",
    "    - Example: Variation of the Learning Rate, Max Depth, Number of Features\n",
    "- [3.3 Grid Search and Random Search](#3.3-Grid-Search-and-Random-Search)\n",
    "    - Grid Search\n",
    "    - Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c18a7-9b9a-4c31-8c88-6250da01c371",
   "metadata": {},
   "source": [
    "## 3.1 Manual Hyperparameter Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40bf54d-c865-4892-9434-ca9b097fe2af",
   "metadata": {},
   "source": [
    "Manual or systematic parameter tuning can significantly improve the results, but it can be also time consuming. So we need to choose according to the application.\n",
    "\n",
    "In general, XGBoost hyperparameters are modifiable via the `params` dictionary. Also, note that the parameters in the `cv()` or `train()` APIs are also hyperparameters! Especially, the `num_boost_round` value, which specifies the number of weak learners or boosting rounds is essential. We can also use the Scikit-Learn API; in that case, we can take advantage of `GridSearchCV` or `RandomSearchCV`, but note that the name of the parameters might change, e.g., `num_boost_round` becomes `n_estimators`. For more, check the documentation: [Python API Reference](https://xgboost.readthedocs.io/en/stable/python/python_api.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8635bd43-1364-4671-984c-308def024cf4",
   "metadata": {},
   "source": [
    "### Example of Manual Parameter Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5515804-9868-46bf-97bf-9ce9f5d92310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781ad4a2-6047-4835-ae8b-1b6b7006dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv(\"../data/ames_housing_trimmed_processed.csv\")\n",
    "X = housing_data[housing_data.columns.tolist()[:-1]]\n",
    "y = housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ac07c32-3288-4698-b798-0d601528bc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manually set paramater values (not default ones)\n",
    "params = {\"objective\":\"reg:squarederror\",\n",
    "          'colsample_bytree': 0.3,\n",
    "          'learning_rate': 0.1,\n",
    "          'max_depth': 5}\n",
    "cv_results_rmse = xgb.cv(dtrain=housing_dmatrix,\n",
    "                         params=params,\n",
    "                         nfold=4,\n",
    "                         num_boost_round=200, # THIS IS ALSO A PARAM!\n",
    "                         metrics=\"rmse\",\n",
    "                         as_pandas=True,\n",
    "                         seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84127d75-9335-4aab-8a4c-beabd71d0ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned rmse: 30370.552735\n"
     ]
    }
   ],
   "source": [
    "print(\"Tuned rmse: %f\" %((tuned_cv_results_rmse[\"test-rmse-mean\"]).tail(1))) # 30370"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a41376-a042-47da-a1f3-3e4ae745918a",
   "metadata": {},
   "source": [
    "### Effect of Varying a Hyperparameter: Number of Boosting Rounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3fe2862-6fea-4d25-bf16-c0ed4d2a024b",
   "metadata": {},
   "source": [
    "In this example, we try different number of boosting rounds, i.e., `num_boost_round`; these denote the number of weak learners under the hood. Note that we can use a loop with any kind of hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d1b1982-ebe0-4c5e-94bf-764fbdba7a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of number of boosting rounds\n",
    "num_rounds = [150, 200, 250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afd58760-1343-44a9-969f-80f16882937e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty list to store final round rmse per XGBoost model\n",
    "final_rmse_per_round = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "831a16f1-2750-4391-a7f9-caf51757ab6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
    "for curr_num_rounds in num_rounds:\n",
    "\n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                        params=params,\n",
    "                        nfold=3,\n",
    "                        num_boost_round=curr_num_rounds, # Several values tried in loop\n",
    "                        metrics=\"rmse\",\n",
    "                        as_pandas=True,\n",
    "                        seed=123)\n",
    "    \n",
    "    # Append final round RMSE\n",
    "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93535d20-d39c-4802-8d4d-689b7e79e6e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   num_boosting_rounds          rmse\n",
      "0                  150  29763.123698\n",
      "1                  200  29634.996745\n",
      "2                  250  29639.554036\n"
     ]
    }
   ],
   "source": [
    "# Print the resultant DataFrame\n",
    "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
    "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7015715-1222-4de2-94a7-3078acf5ef00",
   "metadata": {},
   "source": [
    "### Automated Selection with Early Stopping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ba362-959b-44a1-85e3-0c7ed7b24b69",
   "metadata": {},
   "source": [
    "We can activate activate early stopping with `early_stopping_rounds`: boosting rounds can be stopped before completing the total number of boosting rounds given with `num_boost_round`. The validation metric needs to improve at least once in every `early_stopping_rounds` round(s) to avoid stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "16fdb4ee-0059-46ad-85ee-5efb77c5a0ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter dictionary for each tree: params\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "329cc523-7927-434c-8064-656e8e9ab013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform cross-validation with early stopping: cv_results\n",
    "cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                         params=params,\n",
    "                         nfold=3,\n",
    "                         num_boost_round=50,\n",
    "                         early_stopping_rounds=10,\n",
    "                         metrics=\"rmse\",\n",
    "                         as_pandas=True,\n",
    "                         seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "34d18bdd-2044-4e7b-87ff-fe823cf91050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    train-rmse-mean  train-rmse-std  test-rmse-mean  test-rmse-std\n",
      "0     141871.635417      403.636200   142640.651042     705.559164\n",
      "1     103057.036458       73.769561   104907.664062     111.112417\n",
      "2      75975.963542      253.726946    79262.054687     563.764349\n",
      "3      57420.531250      521.656754    61620.135417    1087.693857\n",
      "4      44552.955729      544.170190    50437.561198    1846.446330\n",
      "5      35763.949219      681.795751    43035.661458    2034.469207\n",
      "6      29861.464193      769.571238    38600.880208    2169.796232\n",
      "7      25994.676432      756.520565    36071.817708    2109.795430\n",
      "8      23306.835937      759.237670    34383.184896    1934.546688\n",
      "9      21459.769531      745.624998    33509.141276    1887.375284\n",
      "10     20148.721354      749.612769    32916.809245    1850.893589\n",
      "11     19215.382161      641.388291    32197.832682    1734.456935\n",
      "12     18627.389323      716.256596    31770.852865    1802.155484\n",
      "13     17960.694661      557.043073    31482.782552    1779.123767\n",
      "14     17559.736979      631.412969    31389.990234    1892.319927\n",
      "15     17205.712891      590.171393    31302.882162    1955.165902\n",
      "16     16876.571940      703.631755    31234.059896    1880.707172\n",
      "17     16597.662110      703.677609    31318.348308    1828.860391\n",
      "18     16330.460937      607.274494    31323.634766    1775.909567\n",
      "19     16005.972982      520.470911    31204.134766    1739.075860\n",
      "20     15814.301432      518.604477    31089.862630    1756.021674\n",
      "21     15493.405924      505.615987    31047.996094    1624.673955\n",
      "22     15270.734049      502.019527    31056.916015    1668.042812\n",
      "23     15086.381836      503.912899    31024.983724    1548.985354\n",
      "24     14917.608724      486.206468    30983.684896    1663.130201\n",
      "25     14709.590169      449.668438    30989.477214    1686.666560\n",
      "26     14457.286133      376.787666    30952.113281    1613.172332\n",
      "27     14185.567057      383.101961    31066.902344    1648.534310\n",
      "28     13934.066732      473.465449    31095.641276    1709.225654\n",
      "29     13749.644857      473.670302    31103.886719    1778.879529\n",
      "30     13549.836914      454.898923    30976.085938    1744.515079\n",
      "31     13413.485351      399.603618    30938.469401    1746.052597\n",
      "32     13275.916016      415.408786    30931.000651    1772.469906\n",
      "33     13085.878581      493.792860    30929.057292    1765.540659\n",
      "34     12947.181315      517.790039    30890.630208    1786.511479\n",
      "35     12846.027344      547.732372    30884.492839    1769.728719\n",
      "36     12702.379232      505.523221    30833.542318    1691.002985\n",
      "37     12532.244141      508.298241    30856.687500    1771.445978\n",
      "38     12384.055013      536.225042    30818.016927    1782.784630\n",
      "39     12198.444010      545.165502    30839.392578    1847.325597\n",
      "40     12054.583333      508.841412    30776.966146    1912.780507\n",
      "41     11897.036458      477.178360    30794.702474    1919.674832\n",
      "42     11756.221354      502.992395    30780.955078    1906.820029\n",
      "43     11618.846029      519.837153    30783.755859    1951.259331\n",
      "44     11484.080078      578.428250    30776.731120    1953.446309\n",
      "45     11356.552734      565.368794    30758.544271    1947.455425\n",
      "46     11193.558594      552.298906    30729.972005    1985.699316\n",
      "47     11071.315429      604.089960    30732.663411    1966.997809\n",
      "48     10950.777995      574.863209    30712.241536    1957.751573\n",
      "49     10824.865885      576.665756    30720.854818    1950.511520\n"
     ]
    }
   ],
   "source": [
    "# Print cv_results\n",
    "# We see the results for each boosting round\n",
    "print(cv_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaea7086-ca2e-4df4-89d6-aae938f84526",
   "metadata": {},
   "source": [
    "## 3.2 Most Common Tunable Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563538ee-7cd8-4d5c-840a-a69b3e771879",
   "metadata": {},
   "source": [
    "**IMPORTANT**: Have a look at the [Python API](https://xgboost.readthedocs.io/en/stable/python/python_api.html) to see all the parameters for any API (i.e., Scikit-Learn or Learning). In the following, the most common parameters are listed.\n",
    "\n",
    "Tree weak learner:\n",
    "\n",
    "- `eta` or `learning_rate`: how quickly we fit the residual error. High values lead to quicker fits.\n",
    "- `gamma`: min loss reduction to create new tree split. Higher value, less splits, less complexity, less overfitting.\n",
    "- `lambda`: L2 reg on leaf weights. Higher value, less complexity.\n",
    "- `alpha`: L1 reg on leaf weights. Higher value, less complexity.\n",
    "- `max_depth`: max depth per tree; how deep each tree is allowed to grow in each round. Higher value, **more** complexity.\n",
    "- `subsample`: fraction of total samples used per tree; in each boosting round, a tree takes one subset of all data points, this value refers to the size of this subset. Higher value, **more** complexity. \n",
    "- `colsample_bytree`: fraction of features used per each tree or boosting round. Not all features need to be used by each weak learner or boosting round. This value refers to how many from the total amount are used, selected randomly. A low value of this parameter is like more regularization.\n",
    "\n",
    "Linear weak learner (much less hyperparameters):\n",
    "\n",
    "- `lambda`: L2 reg on weights. Higher value, less complexity.\n",
    "- `alpha`: L1 reg on weights. Higher value, less complexity.\n",
    "- `lambda_bias`: L2 reg term on bias. Higher value, less complexity.\n",
    "\n",
    "For any type base/weak learner, recall that we can tune the number of boostings or weak learners we want in the `cv()` or `train()` call:\n",
    "\n",
    "- `num_boost_round`\n",
    "- `early_stopping_rounds`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7aa0e-5429-47b6-8258-5be5ba53e57d",
   "metadata": {},
   "source": [
    "### Example: Variation of the Learning Rate, Max Depth, Number of Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec1a0bc6-2523-4c1c-8f94-e514d63ad9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     eta      best_rmse\n",
      "0  0.001  195736.411458\n",
      "1  0.010  179932.192708\n",
      "2  0.100   79759.411458\n"
     ]
    }
   ],
   "source": [
    "# Create your housing DMatrix: housing_dmatrix\n",
    "housing_dmatrix = xgb.DMatrix(data=X, label=y)\n",
    "\n",
    "# Create the parameter dictionary for each tree (boosting round)\n",
    "params = {\"objective\":\"reg:squarederror\", \"max_depth\":3}\n",
    "\n",
    "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
    "vals = [0.001, 0.01, 0.1] # eta\n",
    "#vals = [2, 5, 10, 20] # max_depth\n",
    "#vals = [0.1, 0.5, 0.8, 1] # colsample_bytree\n",
    "best_rmse = []\n",
    "\n",
    "# Systematically vary the eta \n",
    "for curr_val in vals:\n",
    "\n",
    "    params[\"eta\"] = curr_val\n",
    "    #params[\"max_depth\"] = curr_val\n",
    "    #params[\"colsample_bytree\"] = curr_val\n",
    "    \n",
    "    # Perform cross-validation: cv_results\n",
    "    cv_results = xgb.cv(dtrain=housing_dmatrix,\n",
    "                         params=params,\n",
    "                         nfold=3,\n",
    "                         num_boost_round=10,\n",
    "                         early_stopping_rounds=5,\n",
    "                         metrics=\"rmse\",\n",
    "                         as_pandas=True,\n",
    "                         seed=123)\n",
    "\n",
    "    # Append the final round rmse to best_rmse\n",
    "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
    "\n",
    "# Print the resultant DataFrame\n",
    "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84919b4c-9805-428d-9543-10fd7e1cff2f",
   "metadata": {},
   "source": [
    "## 3.3 Grid Search and Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66685f20-f041-4431-a01c-79df49c923f4",
   "metadata": {},
   "source": [
    "We can use `GridSearchCV` and `RandomSearchCV` from Scikit-Learn to systematically obtain the best parameters. To that end, we need to use the Scikit-Learn API, i.e., we instantiate `XGBRegressor` or `XGBClassifier` and user the parameters typical from Scikit-Learn. Note that the parameter seach space increases exponentially as we add parameters, so:\n",
    "\n",
    "- With `GridSearchCV` we might require much more time to find the optimum parameter set.\n",
    "- With `RandomSearchCV` we limit the number of sets, but these are random!\n",
    "\n",
    "There are more advanced techniques for hyperparameter tuning, such as [Bayesian hyperparameter optimization](https://machinelearningmastery.com/scikit-optimize-for-hyperparameter-tuning-in-machine-learning/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95e20ff-d657-4062-9dd9-5b76633dd63c",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "23b5fcfb-b086-4b2b-8eb5-fa5a71ee65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9445811e-35db-44f6-b23b-2dee72405987",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_data = pd.read_csv(\"../data/ames_housing_trimmed_processed.csv\")\n",
    "X = housing_data[housing_data.columns.tolist()[:-1]]\n",
    "y = housing_data[housing_data.columns.tolist()[-1]]\n",
    "housing_dmatrix = xgb.DMatrix(data=X,label=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1dc2f5ff-0fa5-4dec-bdaa-ef56de8bba1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The parameter names with the Scikit-Learn API are different\n",
    "# eta -> learning_rate\n",
    "# num_boost_round -> n_estimators\n",
    "gbm_param_grid = {'learning_rate': [0.01,0.1,0.5,0.9],\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample': [0.3, 0.5, 0.9]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "64162e94-fdb5-4979-a254-a395feefe0aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "gbm = xgb.XGBRegressor()\n",
    "grid_mse = GridSearchCV(estimator=gbm,\n",
    "                        param_grid=gbm_param_grid,\n",
    "                        scoring='neg_mean_squared_error', # negative MSE\n",
    "                        cv=4,\n",
    "                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "02c4d86c-8eae-4413-a06c-94f35ad2dfa4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 12 candidates, totalling 48 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=4,\n",
       "             estimator=XGBRegressor(base_score=None, booster=None,\n",
       "                                    colsample_bylevel=None,\n",
       "                                    colsample_bynode=None,\n",
       "                                    colsample_bytree=None,\n",
       "                                    enable_categorical=False, gamma=None,\n",
       "                                    gpu_id=None, importance_type=None,\n",
       "                                    interaction_constraints=None,\n",
       "                                    learning_rate=None, max_delta_step=None,\n",
       "                                    max_depth=None, min_child_weight=None,\n",
       "                                    missing=nan, monotone_constraints=None,\n",
       "                                    n_estimators=100, n_jobs=None,\n",
       "                                    num_parallel_tree=None, predictor=None,\n",
       "                                    random_state=None, reg_alpha=None,\n",
       "                                    reg_lambda=None, scale_pos_weight=None,\n",
       "                                    subsample=None, tree_method=None,\n",
       "                                    validate_parameters=None, verbosity=None),\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.5, 0.9],\n",
       "                         'n_estimators': [200], 'subsample': [0.3, 0.5, 0.9]},\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_mse.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d1add96a-1d5c-4f07-8a11-d41125b46e0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'learning_rate': 0.1, 'n_estimators': 200, 'subsample': 0.5}\n",
      "Lowest RMSE found:  29105.179169382693\n"
     ]
    }
   ],
   "source": [
    "print(\"Best parameters found: \", grid_mse.best_params_)\n",
    "# Since we have the negative MSE, we need to compute the RMSE from it\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f95851-3577-4dba-816f-fc9cce3e1645",
   "metadata": {},
   "source": [
    "### Random Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763894f8-f914-4e80-af84-b588d8da7ad7",
   "metadata": {},
   "source": [
    "We define the possible hyperparameter values but unlike in the grid search, here we define a number of possible combinations to be tested. Then, for each trial, the hyperparameter values are chosen randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "3db5d968-8ec6-4599-9087-04cfd05969f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 25 candidates, totalling 100 fits\n",
      "Best parameters found:  {'subsample': 0.4, 'n_estimators': 200, 'learning_rate': 0.2}\n",
      "Lowest RMSE found:  29666.410368346937\n"
     ]
    }
   ],
   "source": [
    "# All possible combinations are: 20 * 1 * 20 = 400\n",
    "# BUT: we limit to n_iter=25 the number of combinations\n",
    "# And we will train each of them 4-fold with CV\n",
    "gbm_param_grid = {'learning_rate': np.arange(0.05,1.05,.05), # arange: 20 values\n",
    "                  'n_estimators': [200],\n",
    "                  'subsample': np.arange(0.05,1.05,.05)} # arange: 20 values\n",
    "\n",
    "gbm = xgb.XGBRegressor()\n",
    "randomized_mse = RandomizedSearchCV(estimator=gbm,\n",
    "                                    param_distributions=gbm_param_grid,\n",
    "                                    n_iter=25, # number of combinations\n",
    "                                    scoring='neg_mean_squared_error',\n",
    "                                    cv=4,\n",
    "                                    verbose=1)\n",
    "\n",
    "randomized_mse.fit(X, y)\n",
    "print(\"Best parameters found: \",randomized_mse.best_params_)\n",
    "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a74153b-a5ef-4f08-b090-cf37366d0743",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

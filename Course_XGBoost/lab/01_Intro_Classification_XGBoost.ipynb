{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f42256d-2064-4ef7-845a-bab19923b865",
   "metadata": {},
   "source": [
    "# 1. Classification with XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111a2dd8-986c-41ba-af89-662021f9cff3",
   "metadata": {},
   "source": [
    "These are my personal notes of the Datacamp course [Extreme Gradient Boosting with XGBoost](https://app.datacamp.com/learn/courses/extreme-gradient-boosting-with-xgboost).\n",
    "\n",
    "The course has 4 main sections:\n",
    "\n",
    "1. **Classification**: the current notebook.\n",
    "2. Regression\n",
    "3. Fine-tuning XGBoost\n",
    "4. Using XGBoost in Pipelines\n",
    "\n",
    "XGBoost is an implementation of the [Gradient Boosting](https://en.wikipedia.org/wiki/Gradient_boosting) algorithm in C++ which has bindings to other languages, such as Python. It has the following properties:\n",
    "\n",
    "- Fast.\n",
    "- Best performance.\n",
    "- Parallelizable, on a computer and across the network. So it can work with huge datasets distributed on several nodes/GPUs.\n",
    "- We can use it for classification and regression.\n",
    "- The [Python API](https://xgboost.readthedocs.io/en/stable/python/python_api.html) is easy to use and has two major flavors or sub-APIs:\n",
    "  - The **Scikit-Learn API**: We instantiate `XGBRegressor()` or `XGBClassifier` and then we can `fit()` and `predict()`, using the typical Scikit-Learn parameters; we can even use those objects with other Scikit-Learn modules, such as `GridSearchCV`.\n",
    "  - The **Learning API**: The native XGBoost Python API requires to convert the dataframes into `DMatrix` objects first; then, we have powerful methods which allow for tuning many parameters: `xgb.cv()`, `xgb.train()`. The native/learning API is very easy to use. **Note: the parameter names are different compared to the Scikit-Learn API!**\n",
    "\n",
    "Classification is the original supervised learning problem addressed by XGBoost, although it can also handle regression problems.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```python\n",
    "# PIP\n",
    "pip install xgboost\n",
    "\n",
    "# Conda: General\n",
    "conda install -c conda-forge py-xgboost\n",
    "\n",
    "# Conda: CPU only\n",
    "conda install -c conda-forge py-xgboost-cpu\n",
    "\n",
    "# Conda: Use NVIDIA GPU: Linux x86_64\n",
    "conda install -c conda-forge py-xgboost-gpu\n",
    "\n",
    "# For tree visualization\n",
    "pip install graphviz\n",
    "```\n",
    "\n",
    "### Table of Contents\n",
    "\n",
    "- [1.1 Introduction: Churn Classification Example](#1.1-Introduction:-Churn-Classification-Example)\n",
    "- [1.2 How Does It Work?](#1.2-How-Does-It-Work?)\n",
    "- [1.3 Cross Validation](#1.3-Cross-Validation)\n",
    "- [1.4 When to Use XGBoost](#1.4-When-to-Use-XGBoost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeb780e-9fe5-4d1e-985c-8ecb615b46f7",
   "metadata": {},
   "source": [
    "## 1.1 Introduction: Churn Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5c9b2376-d89e-4dd3-98a3-00ebc287c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fb8a439-721e-483b-8af6-15f9edea0a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data, split\n",
    "class_data = pd.read_csv(\"../data/ChurnData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40398b53-88e5-4321-895a-19c85491d229",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tenure</th>\n",
       "      <th>age</th>\n",
       "      <th>address</th>\n",
       "      <th>income</th>\n",
       "      <th>ed</th>\n",
       "      <th>employ</th>\n",
       "      <th>equip</th>\n",
       "      <th>callcard</th>\n",
       "      <th>wireless</th>\n",
       "      <th>longmon</th>\n",
       "      <th>...</th>\n",
       "      <th>pager</th>\n",
       "      <th>internet</th>\n",
       "      <th>callwait</th>\n",
       "      <th>confer</th>\n",
       "      <th>ebill</th>\n",
       "      <th>loglong</th>\n",
       "      <th>logtoll</th>\n",
       "      <th>lninc</th>\n",
       "      <th>custcat</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.40</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.482</td>\n",
       "      <td>3.033</td>\n",
       "      <td>4.913</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>33.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>33.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>9.45</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.246</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.497</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.30</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.841</td>\n",
       "      <td>3.240</td>\n",
       "      <td>3.401</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>38.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.05</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.800</td>\n",
       "      <td>3.807</td>\n",
       "      <td>4.331</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>14.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.10</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.960</td>\n",
       "      <td>3.091</td>\n",
       "      <td>4.382</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   tenure   age  address  income   ed  employ  equip  callcard  wireless  \\\n",
       "0    11.0  33.0      7.0   136.0  5.0     5.0    0.0       1.0       1.0   \n",
       "1    33.0  33.0     12.0    33.0  2.0     0.0    0.0       0.0       0.0   \n",
       "2    23.0  30.0      9.0    30.0  1.0     2.0    0.0       0.0       0.0   \n",
       "3    38.0  35.0      5.0    76.0  2.0    10.0    1.0       1.0       1.0   \n",
       "4     7.0  35.0     14.0    80.0  2.0    15.0    0.0       1.0       0.0   \n",
       "\n",
       "   longmon  ...  pager  internet  callwait  confer  ebill  loglong  logtoll  \\\n",
       "0     4.40  ...    1.0       0.0       1.0     1.0    0.0    1.482    3.033   \n",
       "1     9.45  ...    0.0       0.0       0.0     0.0    0.0    2.246    3.240   \n",
       "2     6.30  ...    0.0       0.0       0.0     1.0    0.0    1.841    3.240   \n",
       "3     6.05  ...    1.0       1.0       1.0     1.0    1.0    1.800    3.807   \n",
       "4     7.10  ...    0.0       0.0       1.0     1.0    0.0    1.960    3.091   \n",
       "\n",
       "   lninc  custcat  churn  \n",
       "0  4.913      4.0    1.0  \n",
       "1  3.497      1.0    1.0  \n",
       "2  3.401      3.0    0.0  \n",
       "3  4.331      4.0    0.0  \n",
       "4  4.382      3.0    0.0  \n",
       "\n",
       "[5 rows x 28 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b8a8506-fe9f-4725-9e57-d3612b878535",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = class_data.iloc[:,:-1], class_data.iloc[:,-1].astype(int)\n",
    "X_train, X_test, y_train, y_test= train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d65001-b241-4bbf-88b8-0e1d13466cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost Classifier instance\n",
    "# Parameters:\n",
    "# https://xgboost.readthedocs.io/en/stable/parameter.html\n",
    "# Objective functions:\n",
    "# reg:linear - regression (deprecated)\n",
    "# reg:squarederror - regression\n",
    "# reg:logistic - classification, class label output\n",
    "# binary:logistic - classification, class probability output\n",
    "xg_cl = xgb.XGBClassifier(objective='binary:logistic',\n",
    "                          n_estimators=10,\n",
    "                          seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f4b23c2-19c3-407b-a280-f4060335c75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11:51:45] WARNING: /Users/runner/miniforge3/conda-bld/xgboost-split_1645117948562/work/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n",
       "              gamma=0, gpu_id=-1, importance_type=None,\n",
       "              interaction_constraints='', learning_rate=0.300000012,\n",
       "              max_delta_step=0, max_depth=6, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=10, n_jobs=8,\n",
       "              num_parallel_tree=1, predictor='auto', random_state=123,\n",
       "              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=123,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train/Fit\n",
    "xg_cl.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "265fa170-89b9-4d0f-89e3-2df9b3121549",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "preds = xg_cl.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42e46e1c-1157-45d3-9801-9216801d57ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.750000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate\n",
    "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
    "print(\"Accuracy: %f\" % (accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b60607c-a4fe-4415-88a8-1393d5f426ce",
   "metadata": {},
   "source": [
    "## 1.2 How Does It Work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dfa3939-d2ff-461c-9303-72d93f6b38eb",
   "metadata": {},
   "source": [
    "XGBoost works with *weak* or individual base learners underneath; usually, these are **decision trees**, concretely **CARTs: Classification and Regression Trees**.\n",
    "\n",
    "A decision tree is a binary tree where in each node a feature is used to split the dataset in two; that split is associated to a question. The leaves of the tree contain either a class or a value to be predicted. In particular, CARTs always contain a continuous value in the leaves, which can be used as a classifier value when a threshold is defined.\n",
    "\n",
    "Therefore, XGBoost is an **ensemble learning** method: many models are used to yield a result. The underlying *weak* learners can be any algorithm, as mentioned, although CARTs are usually employed. The *weak* learner needs to be any model which is better than random chance, i.e., >50% accuracy in a binary classification. Then, the XGBoost converts those *weak* learners into **strong learners**: weak/bad effects cancel out and strong/good effects are highlighted.\n",
    "\n",
    "*Weak* learners are trained with **boosting**:\n",
    "\n",
    "- Iteratively learn models on subsets of data.\n",
    "- Weight each weak prediction based on learner's performance.\n",
    "- Combine weighted predictions to obtain a single prediction.\n",
    "\n",
    "The XGBoost implementation allows two weak learners:\n",
    "\n",
    "- The mentioned CART trees; these should be used in most cases, because they capture non-linearities.\n",
    "- Linear learners.\n",
    "\n",
    "Notes on the general boosting algorithm:\n",
    "\n",
    "- Each weak learner is created in a boosting round and it uses a subset of the total dataset.\n",
    "- If we use trees, we can select the number of features to be selected randomly to build the tree.\n",
    "- We can apply regularization is the model is overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8309d43f-7754-4bcd-a743-d06a1b5c7d1d",
   "metadata": {},
   "source": [
    "## 1.3 Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088c17be-e3d8-499a-9abc-d6c0984f973d",
   "metadata": {},
   "source": [
    "We can use cross-validation with XGBoost, but the API usage is a bit different:\n",
    "\n",
    "- We need to define `DMatrix` objects.\n",
    "- We call `xgb.cv()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7a2e345d-b0bb-4ac6-825a-3e166da99cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1cac0faa-6d1c-4339-afa5-ed8b6d5d05dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_data = pd.read_csv(\"../data/ChurnData.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e63ffdd7-dacf-404d-8c4b-e211377654fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMatrix is a specific data structure which accelerates the computations\n",
    "# In the regular API, i.e., without cross-validation, DMatrix is automatically generated\n",
    "# but with cross-validation we need to do it explicitly\n",
    "churn_dmatrix = xgb.DMatrix(data=churn_data.iloc[:,:-1], # X\n",
    "                            label=churn_data.churn) # y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64202ace-ef82-457b-93ca-a53c8afdf68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define params\n",
    "# Objective functions:\n",
    "# reg:linear - regression (deprecated)\n",
    "# reg:squarederror - regression\n",
    "# reg:logistic - classification, class label output\n",
    "# binary:logistic - classification, class probability output\n",
    "params = {\"objective\":\"binary:logistic\",\n",
    "          \"max_depth\":4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b82e9322-351c-4e6a-ae61-263cf7555c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit with CV and get results of CV\n",
    "# Parameters:\n",
    "# https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.cv\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix, # DMatrix\n",
    "                    params=params, # parameters dictionary\n",
    "                    nfold=4, # number of non-overlapping folds\n",
    "                    num_boost_round=10, # number of trees\n",
    "                    metrics=\"error\", # error converts to accuracy; \"rmse\" or \"mae\" for regression\n",
    "                    as_pandas=True) # if we want results as a pandas object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e49d3143-2446-490d-ba2d-8f0f939333bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.705000\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %f\" %((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "667cb5f1-26ee-48c0-a33e-7ee45080f6b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   train-auc-mean  train-auc-std  test-auc-mean  test-auc-std\n",
      "0        0.907307       0.025788       0.694683      0.057410\n",
      "1        0.951466       0.017800       0.720245      0.032604\n",
      "2        0.975673       0.009259       0.722732      0.018837\n",
      "3        0.982302       0.006991       0.735959      0.038124\n",
      "4        0.988113       0.005642       0.732957      0.040420\n",
      "0.732957\n"
     ]
    }
   ],
   "source": [
    "# Perform cross-validation with another metric: AUC\n",
    "cv_results = xgb.cv(dtrain=churn_dmatrix,\n",
    "                    params=params, \n",
    "                    nfold=3,\n",
    "                    num_boost_round=5, \n",
    "                    metrics=\"auc\",\n",
    "                    as_pandas=True,\n",
    "                    seed=123)\n",
    "\n",
    "# Print cv_results\n",
    "print(cv_results)\n",
    "\n",
    "# Print the AUC\n",
    "print((cv_results[\"test-auc-mean\"]).iloc[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0b70d7-23a0-41e5-87b8-4a8859115c15",
   "metadata": {},
   "source": [
    "## 1.4 When to Use XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "550bbfcd-98c3-4945-96bf-87de3983c801",
   "metadata": {},
   "source": [
    "We can use XGBoost:\n",
    "\n",
    "- With large datasets:\n",
    "  - 1000 data-point of less than 100 features each,\n",
    "  - however, as long as the number of features < the number of data-points, everything should be fine.\n",
    "- With numerical or categorical features, or a mixture of both.\n",
    "\n",
    "XGBoost is suboptimal if:\n",
    "\n",
    "- Computer vision, Image recognition (better use deep learning)\n",
    "- NLP (better use deep learning)\n",
    "- When the number of features is larger than the number of data-points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0361b05-f572-441b-8390-619cd700b04d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
